{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0,1,2,3,4,5,6,7,8,9],[0,1,2,3,4,5,6,7,8,9],[0,1,2,3,4,5,6,7,8,9],[0,1,2,3,4,5,6,7,8,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 6, 9],\n",
       "       [0, 3, 6, 9]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[::2,0:10:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : (データ数, チャンネル, 高さ, 幅)の4次元配列からなる入力データ\n",
    "    filter_h : フィルターの高さ\n",
    "    filter_w : フィルターの幅\n",
    "    stride : ストライド\n",
    "    pad : パディング\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2次元配列\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    #切り捨て除算\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    print(f'{filter_w}')\n",
    "    #第一引数：N次元の行列　第二引数：前後の文字詰める量　第三引数：埋め込み方式\n",
    "    #データ数、チャンネル方向にはパディング０、高さと幅の方向には前後にpad分0パディングする\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            #https://qiita.com/MA-fn/items/45a45a7417dfb37a5248\n",
    "            #フィルターの要素ごとに乗算する値を取得し、行に並べたい\n",
    "            #⇒フィルターに応じてとびとびに値を取得する\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "            \n",
    "    #N⇔N C⇔oh fh⇔ow fw⇔C oh⇔fh ow⇔fw →　N*oh*ow*C*fh*fw →(N*oh*ow)*(C*fh*fw)\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.util import im2col\n",
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape) # (9, 75)\n",
    "x2 = np.random.rand(10, 3, 7, 7) # 10 個のデータ\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape) # (90, 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畳み込み層の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #フィルターの個数、フィルターのチャンネル数（奥行）、フィルターの高さ、フィルターの幅\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
    "        \n",
    "        #(N, C, H, W) → (N*oh*ow,C*FH*FW)\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        #フィルター（C*FH*FW）を１列に展開　行＝C*FH*FW\n",
    "        #フィルター枚数が列数FNとなる\n",
    "        col_W = self.W.reshape(FN, -1).T \n",
    "        #(N*oh*ow,FN)\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        #(N*oh*ow,FN)⇒(N,out_h,out_w,C)⇒(N,C,out_h,out_w)番号に対応する軸が入る\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling層の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        #展開(1)\n",
    "        #(N, C, H, W) → (N*oh*ow,C*pool_h*pool_w)\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        #(N*oh*ow,C*pool_h*pool_w)⇒(N*oh*ow*C,pool_h*pool_w)\n",
    "        #プーリング適用領域の幅*高さの長さを列に持つ行列\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        #最大値 各々のプーリング適用領域に対して最大値を取得する\n",
    "        out = np.max(col, axis=1)\n",
    "        \n",
    "        #整形　取得した最大値(N*oh*ow*C,1)⇒(N,oh,ow,C)⇒(N,C,oh,ow)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "単純なCNNの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "# 単純なConvNet\n",
    "# conv - relu - pool - affine - relu - affine - softmax\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28),\\\n",
    "                 conv_param={'filter_num':30, 'filter_size':5,\\\n",
    "                            'pad':0, 'stride':1},\\\n",
    "                hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        #=================================================================================\n",
    "        #input_dim = 入力データの（チャンネル、高さ、幅）の次元\n",
    "        #conv_param = 畳み込み層のハイパーパラメータ\n",
    "        #filter_num = フィルターの数　filter_size = フィルターのサイズ pad = パディング stride = ストライド\n",
    "        #hidden_size = 隠れ層（全結合）のニューロンの数\n",
    "        #output_size = 出力層（全結合）のニューロンの数\n",
    "        #weight_init_std = 初期化の際の重みの標準偏差\n",
    "        #==================================================================================\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        #プーリング層の出力サイズを計算\n",
    "        #プーリング適用領域が2*2で、maxプーリングを行うと畳み込み層の出力数の1/4になる\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "        \n",
    "        #重みパラメータの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        \n",
    "        #レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'],self.params['b1'],conv_param['stride'],conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'],self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'],self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    #推論\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    #損失関数の値を求める\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        #one-hot表現をラベル表現に直す\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"勾配を求める（数値微分）\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 入力データ\n",
    "        t : 教師ラベル\n",
    "        Returns\n",
    "        -------\n",
    "        各層の勾配を持ったディクショナリ変数\n",
    "            grads['W1']、grads['W2']、...は各層の重み\n",
    "            grads['b1']、grads['b2']、...は各層のバイアス\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "    \n",
    "    #誤差逆伝搬法を用いた勾配を求める\n",
    "    def gradient(self, x, t):\n",
    "        #forwardで推論を行った後\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # backwardで誤差逆伝搬、勾配を求める\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "        return grads\n",
    "    \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNISTのデータを用いたCNNの分類実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.299566186193197\n",
      "=== epoch:1, train acc:0.385, test acc:0.349 ===\n",
      "train loss:2.2966349281075766\n",
      "train loss:2.292183985096533\n",
      "train loss:2.286814383440778\n",
      "train loss:2.2819420196161637\n",
      "train loss:2.2679352687314545\n",
      "train loss:2.2458468782144316\n",
      "train loss:2.2279796652495145\n",
      "train loss:2.205772341130686\n",
      "train loss:2.1904901821161915\n",
      "train loss:2.136951992375044\n",
      "train loss:2.1008377885148453\n",
      "train loss:2.044308900570155\n",
      "train loss:1.9259422374524666\n",
      "train loss:1.966730634508688\n",
      "train loss:1.926506821305788\n",
      "train loss:1.810888977102704\n",
      "train loss:1.7212297759218596\n",
      "train loss:1.7180235640307167\n",
      "train loss:1.5993995871768456\n",
      "train loss:1.5654615720456622\n",
      "train loss:1.4177915575492441\n",
      "train loss:1.3768212271015343\n",
      "train loss:1.2306687471370241\n",
      "train loss:1.172596997650912\n",
      "train loss:1.1139450083768654\n",
      "train loss:0.9460925686864372\n",
      "train loss:0.9816454145476718\n",
      "train loss:0.8437136245780463\n",
      "train loss:0.7303004987611239\n",
      "train loss:0.8973182969540281\n",
      "train loss:0.7908609300948933\n",
      "train loss:0.7697281613890725\n",
      "train loss:0.7865782824789423\n",
      "train loss:0.6249574829755011\n",
      "train loss:0.6430120481601017\n",
      "train loss:0.681020060675755\n",
      "train loss:0.7269193578014178\n",
      "train loss:0.5928047939022892\n",
      "train loss:0.643359605174719\n",
      "train loss:0.852304924300914\n",
      "train loss:0.6551380270856363\n",
      "train loss:0.6259583621168077\n",
      "train loss:0.4292618993188126\n",
      "train loss:0.5206835177912957\n",
      "train loss:0.5158571861184198\n",
      "train loss:0.596058605588687\n",
      "train loss:0.45084319969518133\n",
      "train loss:0.6487518530658738\n",
      "train loss:0.443221515487499\n",
      "train loss:0.5469878989307513\n",
      "=== epoch:2, train acc:0.808, test acc:0.804 ===\n",
      "train loss:0.47084316941357174\n",
      "train loss:0.7062774036478013\n",
      "train loss:0.3575558838562115\n",
      "train loss:0.600864544763185\n",
      "train loss:0.41033762665852314\n",
      "train loss:0.386022517720056\n",
      "train loss:0.5042029705049401\n",
      "train loss:0.5024760369945716\n",
      "train loss:0.5200255907755367\n",
      "train loss:0.6102145941178251\n",
      "train loss:0.4027202146663725\n",
      "train loss:0.39480402363746925\n",
      "train loss:0.3058703607284319\n",
      "train loss:0.5039285719529458\n",
      "train loss:0.6339399920929544\n",
      "train loss:0.4946477519566427\n",
      "train loss:0.636230206988463\n",
      "train loss:0.44378004275008887\n",
      "train loss:0.3464289379804594\n",
      "train loss:0.45102608299900804\n",
      "train loss:0.41886442837283766\n",
      "train loss:0.5072368740265397\n",
      "train loss:0.44139641463743495\n",
      "train loss:0.39951008233536023\n",
      "train loss:0.45386072415579276\n",
      "train loss:0.5209000688486749\n",
      "train loss:0.42024633163938335\n",
      "train loss:0.37779392056892014\n",
      "train loss:0.27559588945029473\n",
      "train loss:0.5021645872293806\n",
      "train loss:0.4490559241719915\n",
      "train loss:0.45271211074677287\n",
      "train loss:0.2707717299002278\n",
      "train loss:0.305703076433721\n",
      "train loss:0.3605579701973272\n",
      "train loss:0.40137027023760274\n",
      "train loss:0.14075774878293815\n",
      "train loss:0.5028054587621981\n",
      "train loss:0.298413876106657\n",
      "train loss:0.4912561292174296\n",
      "train loss:0.3596691482842638\n",
      "train loss:0.3125089480986833\n",
      "train loss:0.3069181023159308\n",
      "train loss:0.24100680609602163\n",
      "train loss:0.3868345012809103\n",
      "train loss:0.3816871115982375\n",
      "train loss:0.3189803160274234\n",
      "train loss:0.36769393523074506\n",
      "train loss:0.3468781055678376\n",
      "train loss:0.42135387689593395\n",
      "=== epoch:3, train acc:0.879, test acc:0.861 ===\n",
      "train loss:0.43720391768123745\n",
      "train loss:0.28517167980191793\n",
      "train loss:0.27609262382741184\n",
      "train loss:0.4018527707341062\n",
      "train loss:0.5767763654935933\n",
      "train loss:0.33899226813963856\n",
      "train loss:0.3645996623803691\n",
      "train loss:0.2947718752122194\n",
      "train loss:0.22844774143748908\n",
      "train loss:0.2675896968461425\n",
      "train loss:0.4206652754925699\n",
      "train loss:0.2652246905421099\n",
      "train loss:0.3287445013581733\n",
      "train loss:0.24499943634995566\n",
      "train loss:0.40346187405218986\n",
      "train loss:0.22335385355138143\n",
      "train loss:0.24413699750853074\n",
      "train loss:0.2398209877743392\n",
      "train loss:0.28323393614795084\n",
      "train loss:0.3385847531007154\n",
      "train loss:0.28391869367331163\n",
      "train loss:0.2623340051940999\n",
      "train loss:0.33818656144982656\n",
      "train loss:0.5721968649492206\n",
      "train loss:0.2798521562007406\n",
      "train loss:0.29117019773609965\n",
      "train loss:0.26637234350886296\n",
      "train loss:0.3507676586978748\n",
      "train loss:0.2644817147109277\n",
      "train loss:0.3371317085843547\n",
      "train loss:0.22138609042838941\n",
      "train loss:0.2940685115975948\n",
      "train loss:0.3305749541782436\n",
      "train loss:0.37512687121042426\n",
      "train loss:0.1991508336769418\n",
      "train loss:0.23473443371553748\n",
      "train loss:0.20679898458179136\n",
      "train loss:0.28269873521327565\n",
      "train loss:0.2507203245922502\n",
      "train loss:0.5267482423233713\n",
      "train loss:0.39977142277479916\n",
      "train loss:0.28294741843302057\n",
      "train loss:0.21985285187910947\n",
      "train loss:0.31988964060707514\n",
      "train loss:0.3970550768634673\n",
      "train loss:0.4013794589632477\n",
      "train loss:0.24053102367630824\n",
      "train loss:0.38459063901822965\n",
      "train loss:0.19199399050216126\n",
      "train loss:0.4009145249003689\n",
      "=== epoch:4, train acc:0.901, test acc:0.878 ===\n",
      "train loss:0.17124257297538784\n",
      "train loss:0.20545314588192137\n",
      "train loss:0.2945347637591231\n",
      "train loss:0.2851038571201029\n",
      "train loss:0.30282050665415466\n",
      "train loss:0.29534264866187404\n",
      "train loss:0.14525196586055522\n",
      "train loss:0.3162022133281568\n",
      "train loss:0.2417076532620048\n",
      "train loss:0.3021170330275618\n",
      "train loss:0.21540088132454646\n",
      "train loss:0.15707506108293612\n",
      "train loss:0.3249965960016049\n",
      "train loss:0.3021491251893197\n",
      "train loss:0.2794094259184635\n",
      "train loss:0.25601231009579634\n",
      "train loss:0.24360876452176203\n",
      "train loss:0.2655840921459634\n",
      "train loss:0.31690326703175487\n",
      "train loss:0.2823087857037147\n",
      "train loss:0.1883791188113555\n",
      "train loss:0.26419343065413936\n",
      "train loss:0.3453421819416689\n",
      "train loss:0.37995642827122533\n",
      "train loss:0.23449585687358382\n",
      "train loss:0.3058687831132989\n",
      "train loss:0.21554110699954218\n",
      "train loss:0.1210714888004869\n",
      "train loss:0.1398318927004704\n",
      "train loss:0.31426464795074266\n",
      "train loss:0.3298841015793238\n",
      "train loss:0.11822950466596627\n",
      "train loss:0.24484856909906977\n",
      "train loss:0.21525816686912236\n",
      "train loss:0.22811118316269974\n",
      "train loss:0.19198548575700408\n",
      "train loss:0.23849709376270162\n",
      "train loss:0.11689975293775796\n",
      "train loss:0.2544205959003017\n",
      "train loss:0.18484333474767578\n",
      "train loss:0.19695761993369207\n",
      "train loss:0.2273837077054611\n",
      "train loss:0.3208935311294469\n",
      "train loss:0.35078773518783074\n",
      "train loss:0.159565798803857\n",
      "train loss:0.20020495473540406\n",
      "train loss:0.23114148298266327\n",
      "train loss:0.2694394606893657\n",
      "train loss:0.3891994474819538\n",
      "train loss:0.21251726511368363\n",
      "=== epoch:5, train acc:0.903, test acc:0.889 ===\n",
      "train loss:0.26860723441206535\n",
      "train loss:0.1565560317020881\n",
      "train loss:0.2033844081568507\n",
      "train loss:0.23693151767284115\n",
      "train loss:0.20143215647287863\n",
      "train loss:0.270932623475278\n",
      "train loss:0.1790243919504355\n",
      "train loss:0.20092991360471285\n",
      "train loss:0.3390252692018214\n",
      "train loss:0.16817839125972014\n",
      "train loss:0.330356626538159\n",
      "train loss:0.35244520610828184\n",
      "train loss:0.22875221685618946\n",
      "train loss:0.18427866216762306\n",
      "train loss:0.15832371458446023\n",
      "train loss:0.24473726475737653\n",
      "train loss:0.1722262088037246\n",
      "train loss:0.23379343876891023\n",
      "train loss:0.19551917677308012\n",
      "train loss:0.3250191006848074\n",
      "train loss:0.19590363886263146\n",
      "train loss:0.25249780880709183\n",
      "train loss:0.28943482014021343\n",
      "train loss:0.17509250025780942\n",
      "train loss:0.1282868566715199\n",
      "train loss:0.1548737836171406\n",
      "train loss:0.19339183615922995\n",
      "train loss:0.24218512621031793\n",
      "train loss:0.09259007712803151\n",
      "train loss:0.20317545095306958\n",
      "train loss:0.22890201411694594\n",
      "train loss:0.22397392489618428\n",
      "train loss:0.1608781314239713\n",
      "train loss:0.1457699862024106\n",
      "train loss:0.14621792286680435\n",
      "train loss:0.24350844998387056\n",
      "train loss:0.12430805601824435\n",
      "train loss:0.316041413910264\n",
      "train loss:0.27902232630856466\n",
      "train loss:0.10152050224667589\n",
      "train loss:0.15791961526810533\n",
      "train loss:0.16806947773240233\n",
      "train loss:0.25589056857788317\n",
      "train loss:0.21189980301525607\n",
      "train loss:0.19476684398114052\n",
      "train loss:0.20694605685532155\n",
      "train loss:0.10260035498360778\n",
      "train loss:0.19732236400975947\n",
      "train loss:0.0990131946567451\n",
      "train loss:0.1444779119698633\n",
      "=== epoch:6, train acc:0.934, test acc:0.924 ===\n",
      "train loss:0.19377903889922596\n",
      "train loss:0.17080396235922557\n",
      "train loss:0.2719368191423906\n",
      "train loss:0.23171606374887094\n",
      "train loss:0.10919125193271789\n",
      "train loss:0.16566396009887124\n",
      "train loss:0.19825206335383946\n",
      "train loss:0.2018828085539833\n",
      "train loss:0.2504143376993893\n",
      "train loss:0.21181934459351723\n",
      "train loss:0.19864100333884466\n",
      "train loss:0.13432112808452829\n",
      "train loss:0.17026641770630577\n",
      "train loss:0.14596915224917395\n",
      "train loss:0.17271501262026315\n",
      "train loss:0.19395939952049737\n",
      "train loss:0.08441697919290407\n",
      "train loss:0.1006802956300295\n",
      "train loss:0.08564845974092095\n",
      "train loss:0.1552919093523421\n",
      "train loss:0.08194163315696333\n",
      "train loss:0.07374010196533963\n",
      "train loss:0.2471495478965895\n",
      "train loss:0.18618058275989238\n",
      "train loss:0.17944751025694317\n",
      "train loss:0.12329603507199677\n",
      "train loss:0.15653898881813777\n",
      "train loss:0.08092875786661438\n",
      "train loss:0.17972926098980654\n",
      "train loss:0.2024145726785156\n",
      "train loss:0.2013909081199402\n",
      "train loss:0.09087767733689743\n",
      "train loss:0.09446284724418934\n",
      "train loss:0.15780299972155876\n",
      "train loss:0.20672849795801845\n",
      "train loss:0.24766375150898573\n",
      "train loss:0.22242723048824067\n",
      "train loss:0.24175025198119507\n",
      "train loss:0.12356717180762729\n",
      "train loss:0.08702013610174408\n",
      "train loss:0.19693155121364836\n",
      "train loss:0.16397548344492704\n",
      "train loss:0.0905014540688939\n",
      "train loss:0.10082513453462258\n",
      "train loss:0.15459607736870098\n",
      "train loss:0.19575590129134934\n",
      "train loss:0.11278937282076043\n",
      "train loss:0.2209306961933933\n",
      "train loss:0.14028608857405891\n",
      "train loss:0.1628539806849027\n",
      "=== epoch:7, train acc:0.944, test acc:0.913 ===\n",
      "train loss:0.1750923018734811\n",
      "train loss:0.10446292701010615\n",
      "train loss:0.18057875056557088\n",
      "train loss:0.12299584632760288\n",
      "train loss:0.07984028130134438\n",
      "train loss:0.24680770294744703\n",
      "train loss:0.17200047298242846\n",
      "train loss:0.22594231930618944\n",
      "train loss:0.17224961876013697\n",
      "train loss:0.1371454525009132\n",
      "train loss:0.13871624356604928\n",
      "train loss:0.32088566437152694\n",
      "train loss:0.222784301608438\n",
      "train loss:0.09793410483321605\n",
      "train loss:0.17900125444382087\n",
      "train loss:0.13888021499871578\n",
      "train loss:0.08871051206391745\n",
      "train loss:0.15923932112220393\n",
      "train loss:0.1636078017005654\n",
      "train loss:0.20162120230191038\n",
      "train loss:0.17914935994631986\n",
      "train loss:0.1595452712412307\n",
      "train loss:0.19670507044846097\n",
      "train loss:0.11950455158702585\n",
      "train loss:0.1425903154469509\n",
      "train loss:0.1789852832543309\n",
      "train loss:0.07462630002389789\n",
      "train loss:0.059291741763903276\n",
      "train loss:0.09906148663578045\n",
      "train loss:0.1580540033267304\n",
      "train loss:0.12577694057885763\n",
      "train loss:0.12290664532737675\n",
      "train loss:0.11558514371472357\n",
      "train loss:0.17509150077409946\n",
      "train loss:0.07376605202182449\n",
      "train loss:0.14494427760648748\n",
      "train loss:0.09943489755553139\n",
      "train loss:0.1274677740696246\n",
      "train loss:0.2444751858358744\n",
      "train loss:0.08058799134312164\n",
      "train loss:0.2099801535323843\n",
      "train loss:0.13266779377846244\n",
      "train loss:0.12751277342429299\n",
      "train loss:0.09221599992243906\n",
      "train loss:0.13692149885426042\n",
      "train loss:0.11513610578811345\n",
      "train loss:0.10969859182800042\n",
      "train loss:0.09030714461478734\n",
      "train loss:0.11139315314202623\n",
      "train loss:0.18232041434622165\n",
      "=== epoch:8, train acc:0.952, test acc:0.925 ===\n",
      "train loss:0.20240713409016967\n",
      "train loss:0.09909697437736846\n",
      "train loss:0.18563301074964023\n",
      "train loss:0.1407254274137308\n",
      "train loss:0.27740481688082463\n",
      "train loss:0.09452344474077\n",
      "train loss:0.1902893180557272\n",
      "train loss:0.06511349832967156\n",
      "train loss:0.09225634711909243\n",
      "train loss:0.1533554568550052\n",
      "train loss:0.08213994925268572\n",
      "train loss:0.09590338612589278\n",
      "train loss:0.1587807638501806\n",
      "train loss:0.15607468671288763\n",
      "train loss:0.14684220225084427\n",
      "train loss:0.07480850338823988\n",
      "train loss:0.1426012727559794\n",
      "train loss:0.07369572570300625\n",
      "train loss:0.1623665937752481\n",
      "train loss:0.05744279696355008\n",
      "train loss:0.14784687586280681\n",
      "train loss:0.12194553168027618\n",
      "train loss:0.11913930743907444\n",
      "train loss:0.09047902345916564\n",
      "train loss:0.17658481719740862\n",
      "train loss:0.10648791763277118\n",
      "train loss:0.08650917217339027\n",
      "train loss:0.08670457971663728\n",
      "train loss:0.08175900583245252\n",
      "train loss:0.041510866093664056\n",
      "train loss:0.07887733169960169\n",
      "train loss:0.05525076555451332\n",
      "train loss:0.19862734626042983\n",
      "train loss:0.05448887656804276\n",
      "train loss:0.09388562252899985\n",
      "train loss:0.14907087602387126\n",
      "train loss:0.10485303886003225\n",
      "train loss:0.045077286632738495\n",
      "train loss:0.1514005868119016\n",
      "train loss:0.12907745594202902\n",
      "train loss:0.1206445639300247\n",
      "train loss:0.08373165310743344\n",
      "train loss:0.07267790617318189\n",
      "train loss:0.08117516906350153\n",
      "train loss:0.18852331173449574\n",
      "train loss:0.049008978796180803\n",
      "train loss:0.10868303558963546\n",
      "train loss:0.06041039952921037\n",
      "train loss:0.18160591946355126\n",
      "train loss:0.10369960454099736\n",
      "=== epoch:9, train acc:0.96, test acc:0.94 ===\n",
      "train loss:0.16728329633435177\n",
      "train loss:0.07520815700365477\n",
      "train loss:0.10115488344295188\n",
      "train loss:0.04917677791501165\n",
      "train loss:0.052221508243565105\n",
      "train loss:0.04622268633460713\n",
      "train loss:0.08602113011361022\n",
      "train loss:0.11216683913185933\n",
      "train loss:0.08653889367300088\n",
      "train loss:0.19428526736059304\n",
      "train loss:0.11814702640904129\n",
      "train loss:0.12715151145611517\n",
      "train loss:0.06402573056163079\n",
      "train loss:0.11169921347072878\n",
      "train loss:0.07210406334891686\n",
      "train loss:0.05428883004502255\n",
      "train loss:0.10039792980345394\n",
      "train loss:0.1700582097026577\n",
      "train loss:0.06863151895333736\n",
      "train loss:0.16501148851772685\n",
      "train loss:0.05227853728946892\n",
      "train loss:0.16552877312660272\n",
      "train loss:0.1033638673182433\n",
      "train loss:0.143952729886016\n",
      "train loss:0.0829855079602248\n",
      "train loss:0.06180619097560009\n",
      "train loss:0.05442688132238353\n",
      "train loss:0.06852102836738545\n",
      "train loss:0.12743440319623883\n",
      "train loss:0.1614027376164897\n",
      "train loss:0.04497811572544058\n",
      "train loss:0.105883781471843\n",
      "train loss:0.06017427297159968\n",
      "train loss:0.038027298931092954\n",
      "train loss:0.2849866760772043\n",
      "train loss:0.08505090777316582\n",
      "train loss:0.080245621148219\n",
      "train loss:0.10178225047113992\n",
      "train loss:0.0582305860415833\n",
      "train loss:0.0771475897782671\n",
      "train loss:0.18628178089070363\n",
      "train loss:0.12668284640758676\n",
      "train loss:0.050771424255568166\n",
      "train loss:0.19881524842643916\n",
      "train loss:0.12672832443844803\n",
      "train loss:0.06486582164715102\n",
      "train loss:0.19518553083468362\n",
      "train loss:0.08117822516868715\n",
      "train loss:0.03788223396053617\n",
      "train loss:0.050304589600508376\n",
      "=== epoch:10, train acc:0.969, test acc:0.939 ===\n",
      "train loss:0.10218113516959226\n",
      "train loss:0.05624300105695831\n",
      "train loss:0.07716144948498975\n",
      "train loss:0.170566298554252\n",
      "train loss:0.08056411600666843\n",
      "train loss:0.0777416064075092\n",
      "train loss:0.11672934454046638\n",
      "train loss:0.24034650629429277\n",
      "train loss:0.05157928876221506\n",
      "train loss:0.05891772675299966\n",
      "train loss:0.0764122205034024\n",
      "train loss:0.14973020770438952\n",
      "train loss:0.04409875682398866\n",
      "train loss:0.05705462593638515\n",
      "train loss:0.06326801956234623\n",
      "train loss:0.12043997026083178\n",
      "train loss:0.07780757680881843\n",
      "train loss:0.09481407906130757\n",
      "train loss:0.07859932796270706\n",
      "train loss:0.10439636858060013\n",
      "train loss:0.08444613710125662\n",
      "train loss:0.13873106678637662\n",
      "train loss:0.10449696030522611\n",
      "train loss:0.0415283695552873\n",
      "train loss:0.05788824039295733\n",
      "train loss:0.054035053828617945\n",
      "train loss:0.06721290403199945\n",
      "train loss:0.0818729239727171\n",
      "train loss:0.06377117865801595\n",
      "train loss:0.05929726660454113\n",
      "train loss:0.06320876540042446\n",
      "train loss:0.061515992215441066\n",
      "train loss:0.09622034162899488\n",
      "train loss:0.18834092955143142\n",
      "train loss:0.09596259605697037\n",
      "train loss:0.13709245870577202\n",
      "train loss:0.05465741573741736\n",
      "train loss:0.07811868697932818\n",
      "train loss:0.04768706599286585\n",
      "train loss:0.08952917718594224\n",
      "train loss:0.11441221239153097\n",
      "train loss:0.16080506048829635\n",
      "train loss:0.06865082369243786\n",
      "train loss:0.09468955972969996\n",
      "train loss:0.05555477693273133\n",
      "train loss:0.03861854427093183\n",
      "train loss:0.14699038337469753\n",
      "train loss:0.12080088729216996\n",
      "train loss:0.08013373374279943\n",
      "train loss:0.10967009427102666\n",
      "=== epoch:11, train acc:0.966, test acc:0.944 ===\n",
      "train loss:0.0635933353903005\n",
      "train loss:0.06967491420322804\n",
      "train loss:0.06796820639960267\n",
      "train loss:0.11948262164676489\n",
      "train loss:0.044978054746825695\n",
      "train loss:0.045168917951261156\n",
      "train loss:0.06998817047176187\n",
      "train loss:0.11535049116811467\n",
      "train loss:0.05784305108641813\n",
      "train loss:0.06039721719845747\n",
      "train loss:0.09124971583366442\n",
      "train loss:0.10439762239155312\n",
      "train loss:0.11154784334420288\n",
      "train loss:0.055690019752005854\n",
      "train loss:0.03506093184262405\n",
      "train loss:0.1797618123300152\n",
      "train loss:0.046651659727868305\n",
      "train loss:0.06737782843628407\n",
      "train loss:0.04636444260774204\n",
      "train loss:0.13048644120194483\n",
      "train loss:0.05082865393618894\n",
      "train loss:0.04198117821173536\n",
      "train loss:0.029109559598564982\n",
      "train loss:0.02345379159673677\n",
      "train loss:0.06713202745022645\n",
      "train loss:0.09896954191879699\n",
      "train loss:0.14414512172104346\n",
      "train loss:0.049121189342396315\n",
      "train loss:0.169917311333909\n",
      "train loss:0.03551836238342724\n",
      "train loss:0.07923436886078852\n",
      "train loss:0.09755452775857988\n",
      "train loss:0.06333728862779882\n",
      "train loss:0.12645886963827496\n",
      "train loss:0.02111606805299877\n",
      "train loss:0.06775629616949237\n",
      "train loss:0.17359440135321547\n",
      "train loss:0.05587758432083419\n",
      "train loss:0.031113762229270377\n",
      "train loss:0.07777986219629\n",
      "train loss:0.18409088677196578\n",
      "train loss:0.10736343327901267\n",
      "train loss:0.10628058074231403\n",
      "train loss:0.04776610282437543\n",
      "train loss:0.052536813054818346\n",
      "train loss:0.05309759332380607\n",
      "train loss:0.022266941716568667\n",
      "train loss:0.0919964703803871\n",
      "train loss:0.08804659510413931\n",
      "train loss:0.04254452284224782\n",
      "=== epoch:12, train acc:0.977, test acc:0.955 ===\n",
      "train loss:0.10620628291540907\n",
      "train loss:0.15301039896566074\n",
      "train loss:0.03973512880621059\n",
      "train loss:0.05354659796222164\n",
      "train loss:0.09175230987235856\n",
      "train loss:0.0819315088086422\n",
      "train loss:0.08380373909987596\n",
      "train loss:0.09226661992354188\n",
      "train loss:0.07829283591498996\n",
      "train loss:0.03725678549848265\n",
      "train loss:0.046752708565389434\n",
      "train loss:0.029629356653028346\n",
      "train loss:0.04970307010299415\n",
      "train loss:0.09468040470258592\n",
      "train loss:0.08177407691978508\n",
      "train loss:0.07984001734043447\n",
      "train loss:0.07464299016622308\n",
      "train loss:0.17204710168679518\n",
      "train loss:0.04670275723439409\n",
      "train loss:0.0866807172042061\n",
      "train loss:0.07429646228420742\n",
      "train loss:0.03951876075818939\n",
      "train loss:0.03701682340070032\n",
      "train loss:0.07842606377424623\n",
      "train loss:0.03791519506422207\n",
      "train loss:0.045124277950064115\n",
      "train loss:0.04300832662892369\n",
      "train loss:0.0369009850078973\n",
      "train loss:0.03321833985512644\n",
      "train loss:0.05380539087418487\n",
      "train loss:0.13145337322467723\n",
      "train loss:0.06251275479970858\n",
      "train loss:0.14730853919883524\n",
      "train loss:0.04091812896742122\n",
      "train loss:0.0530802642891635\n",
      "train loss:0.05727852101051279\n",
      "train loss:0.037947140796508315\n",
      "train loss:0.05763358232514799\n",
      "train loss:0.04620452758658183\n",
      "train loss:0.0953659544437492\n",
      "train loss:0.04122083820639166\n",
      "train loss:0.0319277775828396\n",
      "train loss:0.028164994702734557\n",
      "train loss:0.037867529561477625\n",
      "train loss:0.0358266817399372\n",
      "train loss:0.06421190701838413\n",
      "train loss:0.045784764620763106\n",
      "train loss:0.0420161924830964\n",
      "train loss:0.04859585646070767\n",
      "train loss:0.1414368367185914\n",
      "=== epoch:13, train acc:0.977, test acc:0.952 ===\n",
      "train loss:0.07054770006126969\n",
      "train loss:0.023753576720832517\n",
      "train loss:0.04525494191773618\n",
      "train loss:0.02519796940440725\n",
      "train loss:0.045306093569734154\n",
      "train loss:0.06239169509830772\n",
      "train loss:0.034522882223771485\n",
      "train loss:0.04118946992129634\n",
      "train loss:0.06774823828205125\n",
      "train loss:0.08496765132695426\n",
      "train loss:0.03815398484658198\n",
      "train loss:0.029408157086664777\n",
      "train loss:0.04213846388635366\n",
      "train loss:0.031088343725561022\n",
      "train loss:0.06126956105113102\n",
      "train loss:0.04528463202741308\n",
      "train loss:0.04626860018502444\n",
      "train loss:0.024993297165307404\n",
      "train loss:0.06297873745365855\n",
      "train loss:0.050296839437296244\n",
      "train loss:0.03487958149536444\n",
      "train loss:0.01952506138696396\n",
      "train loss:0.022967600263634095\n",
      "train loss:0.018528744712363675\n",
      "train loss:0.0656734516966617\n",
      "train loss:0.030669898921226593\n",
      "train loss:0.04658197610240829\n",
      "train loss:0.052321232329602986\n",
      "train loss:0.10784681758393239\n",
      "train loss:0.07010140202414857\n",
      "train loss:0.024483011392162267\n",
      "train loss:0.03784764227809044\n",
      "train loss:0.03293836271132069\n",
      "train loss:0.07145287129749964\n",
      "train loss:0.07065432572851789\n",
      "train loss:0.05562054691982537\n",
      "train loss:0.04354918548293742\n",
      "train loss:0.09313777695370826\n",
      "train loss:0.03285409406498904\n",
      "train loss:0.07226892404910648\n",
      "train loss:0.04167876309291825\n",
      "train loss:0.04543920274550623\n",
      "train loss:0.0536071557921972\n",
      "train loss:0.02658418241001756\n",
      "train loss:0.03911823521362393\n",
      "train loss:0.05029983222032406\n",
      "train loss:0.04108246148495916\n",
      "train loss:0.026146552721751558\n",
      "train loss:0.04524686235957696\n",
      "train loss:0.03281794045162628\n",
      "=== epoch:14, train acc:0.982, test acc:0.954 ===\n",
      "train loss:0.06148593657660241\n",
      "train loss:0.019512098966565642\n",
      "train loss:0.062429419187314164\n",
      "train loss:0.029094902158687406\n",
      "train loss:0.05802994195498932\n",
      "train loss:0.020288333416313812\n",
      "train loss:0.08152109197749008\n",
      "train loss:0.04123141520531725\n",
      "train loss:0.03238563802547046\n",
      "train loss:0.033274465845252246\n",
      "train loss:0.029097692331421246\n",
      "train loss:0.039936094609030255\n",
      "train loss:0.021357228894888815\n",
      "train loss:0.07401596665043297\n",
      "train loss:0.028941254523342466\n",
      "train loss:0.023001452727964992\n",
      "train loss:0.12740568985468448\n",
      "train loss:0.05843353003432591\n",
      "train loss:0.02290262943898507\n",
      "train loss:0.019491151582488934\n",
      "train loss:0.032446414387882594\n",
      "train loss:0.033058271955759444\n",
      "train loss:0.13994818448858237\n",
      "train loss:0.041800752841617335\n",
      "train loss:0.0662183406839916\n",
      "train loss:0.034105121576537\n",
      "train loss:0.014134220792249366\n",
      "train loss:0.03220817469100668\n",
      "train loss:0.04754507567418829\n",
      "train loss:0.04350859471930861\n",
      "train loss:0.057898511752059435\n",
      "train loss:0.07964853227832154\n",
      "train loss:0.07070087085883382\n",
      "train loss:0.026856690011901\n",
      "train loss:0.01740394203262703\n",
      "train loss:0.05162215331274991\n",
      "train loss:0.07188331894845554\n",
      "train loss:0.02174136665014026\n",
      "train loss:0.04938999275130996\n",
      "train loss:0.02668713337381612\n",
      "train loss:0.04305376572281295\n",
      "train loss:0.048809515133513864\n",
      "train loss:0.022900698653854194\n",
      "train loss:0.02770385994386363\n",
      "train loss:0.022435160201644336\n",
      "train loss:0.025695632862111007\n",
      "train loss:0.02900632442536628\n",
      "train loss:0.01708749495070181\n",
      "train loss:0.030708525891891932\n",
      "train loss:0.028085819358657435\n",
      "=== epoch:15, train acc:0.98, test acc:0.957 ===\n",
      "train loss:0.03796608816472964\n",
      "train loss:0.05816383544858395\n",
      "train loss:0.11215323687107133\n",
      "train loss:0.04830915419926662\n",
      "train loss:0.06904508484777834\n",
      "train loss:0.05034575858878563\n",
      "train loss:0.028030510822559366\n",
      "train loss:0.024234208415030482\n",
      "train loss:0.03643735746603461\n",
      "train loss:0.035411472423253425\n",
      "train loss:0.05624541509487058\n",
      "train loss:0.08849787027539344\n",
      "train loss:0.048619597697556124\n",
      "train loss:0.06503060855554574\n",
      "train loss:0.01312897821413258\n",
      "train loss:0.018346608292170892\n",
      "train loss:0.028295809311165387\n",
      "train loss:0.08470300034503049\n",
      "train loss:0.018483054340088376\n",
      "train loss:0.040809063898646525\n",
      "train loss:0.06639157666742766\n",
      "train loss:0.07457078925333226\n",
      "train loss:0.009882838721500838\n",
      "train loss:0.02116347042798677\n",
      "train loss:0.1220273647093595\n",
      "train loss:0.08693649238331702\n",
      "train loss:0.04713475107744974\n",
      "train loss:0.03160862127591361\n",
      "train loss:0.030821566843402128\n",
      "train loss:0.03330538981620034\n",
      "train loss:0.05100054782233141\n",
      "train loss:0.032434367808399124\n",
      "train loss:0.012988803318316788\n",
      "train loss:0.04206949949548815\n",
      "train loss:0.032150102784597664\n",
      "train loss:0.09564515905740452\n",
      "train loss:0.0220352587010749\n",
      "train loss:0.03233779811767155\n",
      "train loss:0.040353015091357004\n",
      "train loss:0.019577459634107886\n",
      "train loss:0.02125173687197577\n",
      "train loss:0.05796723893612117\n",
      "train loss:0.028809252203200672\n",
      "train loss:0.007524139309718291\n",
      "train loss:0.034885231918455106\n",
      "train loss:0.05618161629297826\n",
      "train loss:0.03759084464260903\n",
      "train loss:0.023980836129459095\n",
      "train loss:0.03553120540797524\n",
      "train loss:0.020801505915158412\n",
      "=== epoch:16, train acc:0.988, test acc:0.956 ===\n",
      "train loss:0.018345818731373417\n",
      "train loss:0.022388333880928705\n",
      "train loss:0.02808571914638599\n",
      "train loss:0.030676111060548964\n",
      "train loss:0.013876212907648011\n",
      "train loss:0.027723759748361214\n",
      "train loss:0.01634295660330103\n",
      "train loss:0.012763458944594665\n",
      "train loss:0.03330617397848255\n",
      "train loss:0.04107117418539986\n",
      "train loss:0.030261400399258523\n",
      "train loss:0.03586688501991199\n",
      "train loss:0.018302668213315613\n",
      "train loss:0.028409871180690705\n",
      "train loss:0.03735299280606594\n",
      "train loss:0.04609555607114854\n",
      "train loss:0.02670420365855094\n",
      "train loss:0.03678277880141969\n",
      "train loss:0.021885197927901376\n",
      "train loss:0.027340277717461777\n",
      "train loss:0.005472583705651183\n",
      "train loss:0.03377307537035705\n",
      "train loss:0.043876707150117406\n",
      "train loss:0.02516576361356598\n",
      "train loss:0.02536425849114325\n",
      "train loss:0.02792884098160076\n",
      "train loss:0.02156280851682974\n",
      "train loss:0.01196888562372727\n",
      "train loss:0.02864511619780991\n",
      "train loss:0.022148386103959958\n",
      "train loss:0.0407072847639747\n",
      "train loss:0.024402596100621173\n",
      "train loss:0.04942226919766864\n",
      "train loss:0.015841050192439504\n",
      "train loss:0.051881533063299005\n",
      "train loss:0.015017237486797534\n",
      "train loss:0.015823766512853735\n",
      "train loss:0.025061496426750297\n",
      "train loss:0.0496445915112518\n",
      "train loss:0.03399416959493888\n",
      "train loss:0.0836169921781883\n",
      "train loss:0.030057862257709816\n",
      "train loss:0.020067004550049537\n",
      "train loss:0.03102811790729555\n",
      "train loss:0.014669299982786625\n",
      "train loss:0.061953916248687985\n",
      "train loss:0.025582124295517778\n",
      "train loss:0.04942357075060108\n",
      "train loss:0.017029646708139223\n",
      "train loss:0.03404344706376969\n",
      "=== epoch:17, train acc:0.989, test acc:0.953 ===\n",
      "train loss:0.013553797749095532\n",
      "train loss:0.0360846090411699\n",
      "train loss:0.020790190073921754\n",
      "train loss:0.014988995559122111\n",
      "train loss:0.038125819413179995\n",
      "train loss:0.029492324498290628\n",
      "train loss:0.016846434157172033\n",
      "train loss:0.017739739837705378\n",
      "train loss:0.011481956301352337\n",
      "train loss:0.013017753617253791\n",
      "train loss:0.015483462519357158\n",
      "train loss:0.03783551958016512\n",
      "train loss:0.02675300985823703\n",
      "train loss:0.018200692441389534\n",
      "train loss:0.04230948055973596\n",
      "train loss:0.03826127958236941\n",
      "train loss:0.010423445431589502\n",
      "train loss:0.02286169138227122\n",
      "train loss:0.031631031276820785\n",
      "train loss:0.04475937430990627\n",
      "train loss:0.03225943436633444\n",
      "train loss:0.014511188780352053\n",
      "train loss:0.01912044016297346\n",
      "train loss:0.04007198558690762\n",
      "train loss:0.02246758505031599\n",
      "train loss:0.0374963594180705\n",
      "train loss:0.04764134959135398\n",
      "train loss:0.02805542833274088\n",
      "train loss:0.03939495477640811\n",
      "train loss:0.015900454589366828\n",
      "train loss:0.018621549072290224\n",
      "train loss:0.02721387041606581\n",
      "train loss:0.028955631598394527\n",
      "train loss:0.03458328817360625\n",
      "train loss:0.04080752911980144\n",
      "train loss:0.03368845561207867\n",
      "train loss:0.011022620203419951\n",
      "train loss:0.036013720329139375\n",
      "train loss:0.031477451665160895\n",
      "train loss:0.012213740232777553\n",
      "train loss:0.02905026356859251\n",
      "train loss:0.04210668961690473\n",
      "train loss:0.039777064524908305\n",
      "train loss:0.01763095954533889\n",
      "train loss:0.023679714363971222\n",
      "train loss:0.01014254551022021\n",
      "train loss:0.018051614847188785\n",
      "train loss:0.02538747761513225\n",
      "train loss:0.028918437302570633\n",
      "train loss:0.02957611660492029\n",
      "=== epoch:18, train acc:0.99, test acc:0.957 ===\n",
      "train loss:0.02732628190626751\n",
      "train loss:0.012591175031034267\n",
      "train loss:0.02451005976510472\n",
      "train loss:0.009919612128292498\n",
      "train loss:0.023624490422816183\n",
      "train loss:0.006419045570463406\n",
      "train loss:0.024037043429771424\n",
      "train loss:0.01141783613912509\n",
      "train loss:0.011270278237903891\n",
      "train loss:0.02394160902157625\n",
      "train loss:0.011851754990498824\n",
      "train loss:0.01097994317954424\n",
      "train loss:0.05498702470204424\n",
      "train loss:0.07672671889458257\n",
      "train loss:0.032799858135974976\n",
      "train loss:0.018108440897856574\n",
      "train loss:0.011220816335745948\n",
      "train loss:0.030891878366055656\n",
      "train loss:0.01782115870130775\n",
      "train loss:0.020859391366708387\n",
      "train loss:0.03576197713860519\n",
      "train loss:0.030508248265905227\n",
      "train loss:0.015011192419756882\n",
      "train loss:0.008232431356230148\n",
      "train loss:0.03199003759521531\n",
      "train loss:0.02825743474332533\n",
      "train loss:0.025512444721721747\n",
      "train loss:0.02590803512419321\n",
      "train loss:0.04004703647723426\n",
      "train loss:0.017289807581935882\n",
      "train loss:0.015562367916753797\n",
      "train loss:0.015200116196606413\n",
      "train loss:0.014789681456187904\n",
      "train loss:0.02016370469268076\n",
      "train loss:0.013259769425576344\n",
      "train loss:0.017342420766617853\n",
      "train loss:0.027849552041574424\n",
      "train loss:0.016340738125875366\n",
      "train loss:0.033911323824842264\n",
      "train loss:0.027667271603096504\n",
      "train loss:0.03932955701391582\n",
      "train loss:0.014465146569081553\n",
      "train loss:0.01631437171459664\n",
      "train loss:0.01077554181679359\n",
      "train loss:0.012482479928483583\n",
      "train loss:0.007908333736916521\n",
      "train loss:0.02315913131228878\n",
      "train loss:0.01569379995709517\n",
      "train loss:0.018776161155501014\n",
      "train loss:0.003952918840658662\n",
      "=== epoch:19, train acc:0.995, test acc:0.957 ===\n",
      "train loss:0.01126046577943143\n",
      "train loss:0.046785408432756775\n",
      "train loss:0.009578798733028648\n",
      "train loss:0.012818342408446386\n",
      "train loss:0.01175687846426653\n",
      "train loss:0.01834449943244516\n",
      "train loss:0.043910547971058984\n",
      "train loss:0.012355125784408743\n",
      "train loss:0.00830493446537735\n",
      "train loss:0.012495412387939242\n",
      "train loss:0.026572853363619458\n",
      "train loss:0.02921133639052173\n",
      "train loss:0.014884799422086368\n",
      "train loss:0.021855469267298346\n",
      "train loss:0.00851467283074253\n",
      "train loss:0.06823234543771664\n",
      "train loss:0.01426755403081693\n",
      "train loss:0.007756149102305465\n",
      "train loss:0.01859309456299445\n",
      "train loss:0.017547481032905005\n",
      "train loss:0.0157103280296619\n",
      "train loss:0.03035601009246123\n",
      "train loss:0.01373454603407383\n",
      "train loss:0.022950107879450142\n",
      "train loss:0.023678712524519762\n",
      "train loss:0.021526115432253277\n",
      "train loss:0.023008348976506037\n",
      "train loss:0.05072576272896384\n",
      "train loss:0.007182684819218117\n",
      "train loss:0.04264334069956924\n",
      "train loss:0.023514372822988766\n",
      "train loss:0.049613654336959884\n",
      "train loss:0.019887034099638035\n",
      "train loss:0.025038803995167468\n",
      "train loss:0.030512133714993842\n",
      "train loss:0.06013295480915515\n",
      "train loss:0.007179829808939685\n",
      "train loss:0.038067812458986376\n",
      "train loss:0.021537628993554363\n",
      "train loss:0.008247511757083055\n",
      "train loss:0.02227279779361726\n",
      "train loss:0.015919964309743868\n",
      "train loss:0.021368530183152407\n",
      "train loss:0.009764798730964617\n",
      "train loss:0.01504226483642231\n",
      "train loss:0.017818926484475847\n",
      "train loss:0.006425868578439641\n",
      "train loss:0.01681145372463706\n",
      "train loss:0.010846567273190768\n",
      "train loss:0.007852194141057627\n",
      "=== epoch:20, train acc:0.992, test acc:0.963 ===\n",
      "train loss:0.014246403147346567\n",
      "train loss:0.018809568854016006\n",
      "train loss:0.02180964685690786\n",
      "train loss:0.008580863874910109\n",
      "train loss:0.0109935883717043\n",
      "train loss:0.022527655182865872\n",
      "train loss:0.045011219202467555\n",
      "train loss:0.020956054555796046\n",
      "train loss:0.00820769497607687\n",
      "train loss:0.03572050717881969\n",
      "train loss:0.024917944106606082\n",
      "train loss:0.02036901791211898\n",
      "train loss:0.015293833529749591\n",
      "train loss:0.03751709418754593\n",
      "train loss:0.007279859648574114\n",
      "train loss:0.01978812308269612\n",
      "train loss:0.009608999918379381\n",
      "train loss:0.012536054020344755\n",
      "train loss:0.012054622296473494\n",
      "train loss:0.015124074043114144\n",
      "train loss:0.009694927090133086\n",
      "train loss:0.03421645286546115\n",
      "train loss:0.013565054602684938\n",
      "train loss:0.029951277917950398\n",
      "train loss:0.049661265562339664\n",
      "train loss:0.02486757803660674\n",
      "train loss:0.013176678904942443\n",
      "train loss:0.013886424852490049\n",
      "train loss:0.015557370634482754\n",
      "train loss:0.00811986351251412\n",
      "train loss:0.01229632133607844\n",
      "train loss:0.0063292418825650985\n",
      "train loss:0.006295688795552944\n",
      "train loss:0.009412965934646045\n",
      "train loss:0.006382729223457854\n",
      "train loss:0.004363636786680522\n",
      "train loss:0.014305427845957003\n",
      "train loss:0.016276807990721457\n",
      "train loss:0.015238929017663307\n",
      "train loss:0.01669292885575216\n",
      "train loss:0.015512897725993574\n",
      "train loss:0.011023212352397718\n",
      "train loss:0.023872903222312218\n",
      "train loss:0.011897435696676416\n",
      "train loss:0.00683934814300453\n",
      "train loss:0.025720344396083613\n",
      "train loss:0.014261971012910872\n",
      "train loss:0.022629854963111002\n",
      "train loss:0.010647136831852644\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.961\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "#from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "from collections import OrderedDict\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 処理に時間のかかる場合はデータを削減 \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "#単純なCNNの設定\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "\n",
    "#ハイパーパラメータの最適化          \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# パラメータの保存\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# グラフの描画\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畳み込みにおける重みの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from simple_convnet import SimpleConvNet\n",
    "\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "# ランダム初期化後の重み\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 学習後の重み\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
